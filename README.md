# ğŸ§  Hierarchical RAG System with LangChain Ã— Ollama

This repository implements a modular, category-aware Retrieval-Augmented Generation (RAG) system built with [LangChain](https://www.langchain.com/) and [Ollama](https://ollama.com/). It supports hierarchical metadata tagging, vector store merging, and dynamic prompt switching for Japanese and English use cases.

## ğŸš€ Features

- ğŸ“ Category-based vector store construction using Markdown files
- ğŸ§  Conversational RAG chains with chat history context
- ğŸ·ï¸ Metadata-aware document retrieval via FAISS
- ğŸ§© Modular chain factory for various RAG strategies (`stuff`, `map_reduce`, etc.)
- ğŸ§ª CLI for experimentation (`rag` or `llm` mode)
- ğŸ“ Document support: Markdown (fully), DOCX/PDF via Docling
- ğŸ“ Dynamic chunking via document structure analysis

## ğŸ“‚ Project Structure

```

.
â”œâ”€â”€ main.py                  # Main entry point to run the RAG CLI
â”œâ”€â”€ RAGSession.py           # Session manager class
â”œâ”€â”€ chain\_factory.py        # Chain builder for LangChain RAG
â”œâ”€â”€ retriever\_utils.py      # FAISS retriever manager and metadata editing
â”œâ”€â”€ process\_utils.py        # Wrapper for document conversion + vectorization
â”œâ”€â”€ vectorization.py        # Vector store generation logic
â”œâ”€â”€ document\_utils.py       # Document loaders and Markdown converters
â”œâ”€â”€ llm\_config.py           # LLM and prompt configuration
â”œâ”€â”€ embedding\_config.py     # Embedding model selection (via Ollama API)
â”œâ”€â”€ interactive\_cli.py      # CLI mode runner (RAG and LLM)
â”œâ”€â”€ prompts.py              # Prompt templates (Japanese/English)
â””â”€â”€ sample/
â”œâ”€â”€ markdown/           # Input Markdown files
â””â”€â”€ vectorstore/        # Output FAISS vectorstores

````

## ğŸ”§ Requirements

- Python 3.10+
- [Ollama](https://ollama.com/) running locally (`ollama serve`)
- LangChain and community extensions
- Docling (for document conversion)

Install dependencies:

```bash
pip install -r requirements.txt
````

Install models:

```bash
ollama pull bge-m3
ollama pull gemma3:4b
````

Start Ollama backend (in another terminal):

```bash
ollama serve
```

## â–¶ï¸ Usage

Run in interactive RAG mode:

```bash
python main.py
```

When prompted:

* Type a question (in Japanese or English depending on prompt)
* Type `exit` to quit the session

## ğŸ·ï¸ Metadata Structure

Each `.faiss` vector store is accompanied by a `metadata.json`:

```json
{
  "embedding_model": "nomic-embed-text:latest",
  "loader_type": "markdown",
  "text_splitter_type": "recursive",
  "category": {
    "tagname": "å¤§å­¦",
    "level": 1
  }
}
```

This metadata is used for selecting relevant documents by `tagname` and `level`.

## ğŸ“š Prompt Options

You can choose from various prompt styles in `llm_config.py`:

* `japanese_concise` â€“ è«–ç†çš„ã‹ã¤ç°¡æ½”ãªæ—¥æœ¬èªå¿œç­”
* `default` â€“ ç°¡æ˜“æ—¥æœ¬èª
* `english_verbose` â€“ Detailed English response
* `rephrase` â€“ Search query rephrasing

## ğŸ“Œ Notes

* `granite-embedding:278m` is known to crash during vectorization. Use `nomic-embed-text:latest`.
* Only `.md` is natively supported. PDF/DOCX will be converted via Docling.

## ğŸ“œ License

MIT License (c) 2025 Yu Fujimoto
