ã¯ã„ã€æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚æ›´æ–°ã•ã‚ŒãŸ `README.md` ã®å†…å®¹ã‚’ä»¥ä¸‹ã«è¡¨ç¤ºã—ã¾ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€ãŠä½¿ã„ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã« `README.md` ã¨ã„ã†åå‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¦ãã ã•ã„ã€‚

```markdown
# ğŸ§  Hierarchical RAG System with LangChain Ã— Ollama (and other tools)

This repository implements a modular, category-aware Retrieval-Augmented Generation (RAG) system built with [LangChain](https://www.langchain.com/) and [Ollama](https://ollama.com/). It supports hierarchical metadata tagging, vector store merging, and dynamic prompt/configuration switching for Japanese and English use cases. The repository also includes a separate module for decision tree analysis and simulation.

## ğŸš€ Features

- ğŸ“ Category-based vector store construction using Markdown and other files
- ğŸ§  Conversational RAG chains with chat history context
- ğŸ·ï¸ Metadata-aware document retrieval via FAISS and potentially database
- ğŸ§© Modular chain factory for various RAG strategies (`stuff`, `map_reduce`, etc.)
- ğŸ§ª CLI for experimentation (`rag` or `llm` mode)
- ğŸ“ Document support: Markdown (fully), DOCX/PDF via Docling integration
- ğŸ“ Dynamic chunking via document structure analysis
- âœ¨ Database integration (e.g., for metadata, history)
- ğŸ”„ Dynamic RAG configuration switching

*Note: Features specifically related to the `decisiontree` module are not listed here.*

## ğŸ“‚ Project Structure

```
.
README.md                    # This file.
LICENSE.txt                  # MIT License.
requirements.txt             # External libraries.
core/                        # Core objects for RAG construction.
 â”œâ”€â”€ chain_factory.py        # Chain builder for LangChain RAG
 â”œâ”€â”€ database/               # Directory for database files
 â”œâ”€â”€ database.py             # Database utilities and interface
 â”œâ”€â”€ document_utils.py       # Document loaders and converters (Markdown, PDF, DOCX)
 â”œâ”€â”€ embedding_config.py     # Embedding model selection (via Ollama API)
 â”œâ”€â”€ llm_config.py           # LLM and prompt configuration
 â”œâ”€â”€ main.py                 # Main entry point to run the RAG CLI
 â”œâ”€â”€ process_utils.py        # Document processing (conversion, vectorization)
 â”œâ”€â”€ prompts.py              # Prompt templates (Japanese/English)
 â”œâ”€â”€ rag_session.py          # RAG session manager class (formerly RAGSession.py)
 â”œâ”€â”€ retriever_utils.py      # FAISS retriever manager and metadata editing
 â”œâ”€â”€ sample/                 # Sample data for RAG.
 â”‚   â”œâ”€â”€ database.db         # Sample SQLite database file
 â”‚   â”œâ”€â”€ markdown/           # Input Markdown files
 â”‚   â”œâ”€â”€ pdf/                # Input PDF files (used with Docling)
 â”‚   â””â”€â”€ vectorstore/        # Output FAISS vectorstores (structured by category and document/section)
 â””â”€â”€ switch_rag_objects.py   # Logic for dynamically switching RAG configurations or sources
decisiontree/                # Separate module for Decision Tree analysis and simulation.
 â”œâ”€â”€ model.py                # Defines decision tree structure and logic
 â”œâ”€â”€ parser.py               # Parses decision tree models from input files
 â”œâ”€â”€ sample/                 # Sample data for the decisiontree module
 â”‚   â”œâ”€â”€ model/              # Sample decision tree model files
 â”‚   â”œâ”€â”€ scenario/           # Sample scenario data for simulations
 â”‚   â””â”€â”€ simulated_results/  # Output directory for simulation results
 â”œâ”€â”€ sample.py               # Example script demonstrating decisiontree usage
 â”œâ”€â”€ simulator.py            # Handles the simulation execution
 â””â”€â”€ utils.py                # Utility functions for the decisiontree module
```
*Note: The `__pycache__` directories and standard `__init__.py` files are omitted for brevity.*

## ğŸ”§ Requirements

- Python 3.10+
- [Ollama](https://ollama.com/) running locally (`ollama serve`)
- LangChain and community extensions
- Docling (for document conversion like PDF/DOCX)
- Dependencies for the `decisiontree` module (check `requirements.txt`)

Install dependencies:

```bash
pip install -r requirements.txt
````

Install models:

```bash
ollama pull nomic-embed-text:latest
ollama pull bge-m3
ollama pull gemma3:4b
````

Start Ollama backend (in another terminal):

```bash
ollama serve
```

## â–¶ï¸ Usage

Run in interactive RAG mode:

```bash
python core/main.py
```

When prompted:

* Type a question (in Japanese or English depending on prompt)
* Type `exit` to quit the session

*Note: Usage instructions for the `decisiontree` module are not covered in this RAG-focused README.*

This metadata is used for selecting relevant documents by `tagname` and `level`, facilitating the hierarchical retrieval.

## ğŸ“š Prompt Options

You can choose from various prompt styles defined in `core/llm_config.py`:

*   `japanese_concise` â€“ è«–ç†çš„ã‹ã¤ç°¡æ½”ãªæ—¥æœ¬èªå¿œç­”
*   `default` â€“ ç°¡æ˜“æ—¥æœ¬èª
*   `english_verbose` â€“ Detailed English response
*   `rephrase` â€“ Search query rephrasing

## ğŸ“Œ Notes

*   `granite-embedding:278m` is known to crash during vectorization. Use `bge-m3` or `nomic-embed-text:latest`.
*   Only `.md` is natively supported for direct loading. PDF/DOCX will be converted via Docling integration.
*   The `decisiontree/` directory contains a separate module unrelated to the core RAG system described in the main body of this README. Refer to its specific documentation or sample scripts (`decisiontree/sample.py`) for usage details.

## ğŸ“œ License

MIT License (c) 2025 Yu Fujimoto
```
