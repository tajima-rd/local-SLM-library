def run_rag_cli(qa_chain):
    """
    å¯¾è©±å‹ CLIï¼šRAG ãƒã‚§ãƒ¼ãƒ³ã‚’åˆ©ç”¨ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    """
    print("ğŸ” RAG å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit'ï¼‰")

    while True:
        query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
        if query.strip().lower() == "exit":
            break

        response = qa_chain.invoke({
            "input": query,
            "chat_history": []
        })

        print("\nğŸ“„ å‚ç…§å…ƒ:")
        for doc in response.get("source_documents", []):
            print("  -", doc.metadata.get("source"))

        print("\nğŸ§  å›ç­”:")
        print(response.get("answer") or response.get("output"))


def run_llm_cli(llm, prompt_template):
    """
    å¯¾è©±å‹ CLIï¼šå˜ãªã‚‹LLMã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã§ç›´æ¥å¿œç­”ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    """
    print("ğŸ’¬ LLM å˜ä½“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit'ï¼‰")

    while True:
        query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
        if query.strip().lower() == "exit":
            break

        prompt = prompt_template.format_messages(
            input=query,
            context="ï¼ˆæ–‡è„ˆãªã—ï¼‰"
        )
        response = llm.invoke(prompt)

        print("\nğŸ§  å›ç­”:")
        print(response)
