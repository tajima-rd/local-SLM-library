# -----------------------------
# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# -----------------------------
import os
import json
import shutil
from uuid import uuid4
from pathlib import Path
from typing import Optional

# -----------------------------
# LangChain ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¾¤
# -----------------------------
# Core
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate  # type: ignore

# Chains
from langchain.chains import (  # type: ignore
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain.chains.llm import LLMChain  # type: ignore
from langchain.chains.combine_documents.stuff import StuffDocumentsChain, create_stuff_documents_chain  # type: ignore
from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain  # type: ignore
from langchain.chains.combine_documents.reduce import ReduceDocumentsChain  # type: ignore

# Embeddings & VectorStore
from langchain_ollama import OllamaEmbeddings  # type: ignore
from langchain_community.vectorstores import FAISS  # type: ignore
from langchain.docstore.document import Document  # type: ignore

# -----------------------------
# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# -----------------------------
from . import retriever_utils
from . import document_utils as docutils

from .retriever_utils import (
    edit_vectorstore_metadata,  # ãƒ¡ã‚¿ç·¨é›†é–¢æ•°
    RetrieverCategory,          # éšå±¤ã‚«ãƒ†ã‚´ãƒªå®šç¾©
)


SUPPORTED_CHAINS = ["conversational", "retrievalqa", "llmchain", "map_reduce", "refine", "stuff"]

def get_chain(
    llm,
    chain_type: str,
    retriever,
    prompt_template: Optional[PromptTemplate] = None,
    chat_history_variable: str = "chat_history",
    k: int = 5,
    **kwargs
):
    """
    æŸ”è»Ÿãªãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰é–¢æ•°ï¼ˆLangChain APIå¯¾å¿œï¼‰ã€‚
    
    Parameters:
    - llm: LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    - chain_type: "conversational", "retrievalqa", "llmchain", "map_reduce", "refine", "stuff" ã®ã„ãšã‚Œã‹
    - retriever: æ¤œç´¢å™¨
    - prompt_template: ä»»æ„ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    - chat_history_variable: ä¼šè©±å±¥æ­´ã®å¤‰æ•°å
    - k: æ¤œç´¢æ•°
    - kwargs: ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæœªä½¿ç”¨ï¼‰
    """
    if chain_type == "conversational":
        return _build_conversational_chain(llm, retriever, prompt_template, chat_history_variable)

    elif chain_type == "retrievalqa":
        return _build_retrieval_qa_chain(llm, retriever, prompt_template)

    elif chain_type == "llmchain":
        if prompt_template is None:
            raise ValueError("prompt_template is required for llmchain")
        return prompt_template | llm  # RunnableSequence ã‚’è¿”ã™

    elif chain_type in ["stuff", "map_reduce", "refine"]:
        return _build_summarize_chain(llm, chain_type, prompt_template)

    else:
        raise ValueError(f"Unsupported chain type: {chain_type}")

def _build_conversational_chain(
    llm,
    retriever,
    prompt_template: Optional[PromptTemplate],
    chat_history_variable: str = "chat_history"
):
    """
    ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸ Conversational Retrieval Chain ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    
    Parameters:
    - llm: è¨€èªãƒ¢ãƒ‡ãƒ«
    - retriever: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å™¨ï¼ˆRetrieverï¼‰
    - prompt_template: combine_documents ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰
    - chat_history_variable: ä¼šè©±å±¥æ­´å¤‰æ•°åï¼ˆdefault: "chat_history"ï¼‰
    
    Returns:
    - LangChain Retrieval Chain
    """
    from prompts import QUESTION_REPHRASE_PROMPT_STR, DEFAULT_COMBINE_PROMPT_STR
    from langchain_core.prompts import ChatPromptTemplate # type: ignore

    # Step 1: æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ History-aware Retriever ã®ä½œæˆ
    question_prompt = ChatPromptTemplate.from_template(QUESTION_REPHRASE_PROMPT_STR).partial(
        chat_history_variable=chat_history_variable
    )

    history_aware_retriever = create_history_aware_retriever(
        retriever=retriever,
        llm=llm,
        prompt=question_prompt
    )

    # Step 2: combine_documents ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    if prompt_template is None:
        prompt_template = ChatPromptTemplate.from_template(DEFAULT_COMBINE_PROMPT_STR)
    
    combine_chain = create_stuff_documents_chain(llm, prompt_template)

    # Step 3: å…¨ä½“ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¦è¿”ã™
    return create_retrieval_chain(history_aware_retriever, combine_chain)

def _build_retrieval_qa_chain(
    llm,
    retriever,
    prompt_template: Optional[PromptTemplate]
):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãª Retrieval QA ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    
    Parameters:
    - llm: è¨€èªãƒ¢ãƒ‡ãƒ«
    - retriever: æ¤œç´¢å™¨
    - prompt_template: combine_documents ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆNone ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰

    Returns:
    - LangChain Retrieval Chain
    """
    if prompt_template is None:
        prompt_template = DEFAULT_COMBINE_PROMPT

    combine_chain = create_stuff_documents_chain(llm, prompt_template)
    return create_retrieval_chain(retriever, combine_chain)

def _build_summarize_chain(
    llm,
    chain_type: str,
    prompt_template: Optional[PromptTemplate]
):
    """
    è¦ç´„ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚stuff / map_reduce / refine ã®3ç¨®ã«å¯¾å¿œã€‚

    Parameters:
    - llm: è¨€èªãƒ¢ãƒ‡ãƒ«
    - chain_type: "stuff", "map_reduce", "refine"
    - prompt_template: combine_documents ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰

    Returns:
    - æ–‡æ›¸è¦ç´„ãƒã‚§ãƒ¼ãƒ³
    """
    if prompt_template is None:
        prompt_template = ChatPromptTemplate.from_template("""
        ä»¥ä¸‹ã®è¤‡æ•°æ–‡æ›¸ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š

        {context}
        """)

    if chain_type == "stuff":
        return create_stuff_documents_chain(llm, prompt_template)
    elif chain_type == "map_reduce":
        return _build_map_reduce_chain(llm, prompt_template)
    elif chain_type == "refine":
        return create_refine_documents_chain(llm, prompt_template)
    else:
        raise ValueError(f"Unknown summarize chain type: {chain_type}")

def _build_map_reduce_chain(
    llm,
    prompt_template: PromptTemplate
):
    """
    MapReduce å½¢å¼ã®è¦ç´„ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

    Parameters:
    - llm: è¨€èªãƒ¢ãƒ‡ãƒ«
    - prompt_template: PromptTemplate ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

    Returns:
    - MapReduceDocumentsChain
    """
    # LLM ãƒã‚§ãƒ¼ãƒ³ï¼ˆMapping ãƒ•ã‚§ãƒ¼ã‚ºï¼‰
    llm_chain = LLMChain(llm=llm, prompt=prompt_template)

    # Stuff ãƒã‚§ãƒ¼ãƒ³ï¼ˆæ–‡æ›¸çµ±åˆç”¨ï¼‰
    stuff_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_prompt=prompt_template,
        document_variable_name="context"
    )

    # Reduce ãƒã‚§ãƒ¼ãƒ³ï¼ˆè¦ç´„ã®é›†ç´„ï¼‰
    reduce_chain = ReduceDocumentsChain(
        combine_documents_chain=stuff_chain
    )

    # MapReduce ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    return MapReduceDocumentsChain(
        llm_chain=llm_chain,
        reduce_documents_chain=reduce_chain
    )

def prepare_chain_from_path(
    llm,
    faiss_paths: list[Path],
    chain_type: str = "conversational",
    k: int = 5,
    prompt_template: Optional[PromptTemplate] = None,
):
    """
    æŒ‡å®šãƒ‘ã‚¹é…ä¸‹ã®ã™ã¹ã¦ã® FAISS ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’çµ±åˆã—ã€Retriever ãŠã‚ˆã³ RAG ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

    Parameters:
    - llm: è¨€èªãƒ¢ãƒ‡ãƒ«
    - base_path: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ.faiss ã‚’å«ã‚€ãƒ•ã‚©ãƒ«ãƒ€ç¾¤ã®è¦ªï¼‰
    - chain_type: ãƒã‚§ãƒ¼ãƒ³ã®ç¨®é¡ï¼ˆ"conversational", "retrievalqa", ãªã©ï¼‰
    - k: æ¤œç´¢æ•°ï¼ˆRetrieverç”¨ï¼‰
    - prompt_template: combine_documents ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»»æ„ï¼‰

    Returns:
    - LangChain ãƒã‚§ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    if not faiss_paths:
        raise FileNotFoundError(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆ.faissï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print(f"ğŸ” {len(faiss_paths)} ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’çµ±åˆã—ã¾ã™: {[str(p) for p in faiss_paths]}")

    vectorstore = retriever_utils.merge_vectorstore([str(p) for p in faiss_paths])
    retriever = retriever_utils.create_retriever(vectorstore, k=k)

    return get_chain(
        llm=llm,
        chain_type=chain_type,
        retriever=retriever,
        prompt_template= prompt_template,
        k=k,
    )


def save_chain_from_markdown(
    md_path: str,
    vect_path: str,
    embedding_name: str,
    category: RetrieverCategory,
    loader_type: str = "markdown",
):
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    embeddings = OllamaEmbeddings(model=embedding_name)

    documents = docutils.load_documents(md_path, loader_type)

    print("ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚")
    splitter = docutils.suggest_text_splitter(
        documents=documents,
        loader_type=loader_type
    )
    split_docs = splitter.split_documents(documents)

    for doc in split_docs:
        doc.metadata["doc_id"] = str(uuid4())

        existing_category = doc.metadata.get("category", {})
        if not isinstance(existing_category, dict):
            existing_category = {}

        doc.metadata["category"] = category.to_dict()

    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(vect_path)

    metadata = {
        "embedding_model": embeddings.model,
        "loader_type": loader_type,
        "category": category.to_dict(),
    }
    with open(os.path.join(vect_path, "metadata.json"), "w", encoding="utf-8") as f:
        json.dump(metadata, f)

    return True

def save_chain_from_text(
    text: str,
    vect_path: str,
    embedding_name: str,
    category: RetrieverCategory,
    loader_type: str = "text",  # æ˜ç¤ºã—ã¦ãŠãã¨ã‚ˆã„
) -> bool:
    """
    ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã€FAISS ã«ä¿å­˜ã™ã‚‹ã€‚
    """

    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    embeddings = OllamaEmbeddings(model=embedding_name)

    # Documentã«å¤‰æ›ï¼ˆmetadataã¯ã‚ã¨ã§ä»˜ä¸ï¼‰
    document = Document(page_content=text, metadata={})
    documents = [document]

    print("ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚")
    splitter = docutils.suggest_text_splitter(
        documents=documents,
        loader_type=loader_type
    )
    split_docs = splitter.split_documents(documents)

    for doc in split_docs:
        doc.metadata["doc_id"] = str(uuid4())

        existing_category = doc.metadata.get("category", {})
        if not isinstance(existing_category, dict):
            existing_category = {}

        doc.metadata["category"] = category.to_dict()

    # FAISSä¿å­˜
    print(f"[DEBUG] ãƒãƒ£ãƒ³ã‚¯æ•°: {len(split_docs)}")
    print(f"[DEBUG] æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯: {split_docs[0].page_content if split_docs else 'ãªã—'}")

    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(vect_path)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    metadata = {
        "embedding_model": embeddings.model,
        "loader_type": loader_type,
        "category": category.to_dict(),
    }
    with open(os.path.join(vect_path, "metadata.json"), "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    return True