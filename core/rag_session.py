from pathlib import Path
from langchain_core.language_models import BaseLanguageModel # type: ignore

from . import chain_factory
from . import llm_config
from . import retriever_utils
from . import process_utils
from . import ingestion

class RAGSession:
    class VectorStoreEntry:
        def __init__(self, file_path_str: str, category: retriever_utils.RetrieverCategory):
            self.file_path = Path(file_path_str)
            self.category = category

        def __repr__(self):
            return f"VectorStoreEntry(path={self.file_path}, category={self.category})"
        
    def __init__(
        self,
        model_name: str = None,
        default_template: str = None,
        embedding_name: str = None
    ):
        self.model_name = model_name
        self.prompt_template = llm_config.load_prompt(default_template)
        self.embedding_name = embedding_name
        self.llm: BaseLanguageModel = llm_config.load_llm(model_name)
        self.qa_chain = None

        print(f"DEBUG RAGSession: Loaded prompt template string: {self.prompt_template}")
        if self.prompt_template is None:
            print("DEBUG RAGSession: Failed to load prompt template!")

    def build_vectorstore(
            self,
            entries: list[VectorStoreEntry],
            markdown_dir: Path,
            overwrite: bool):
        """
        å„ VectorStoreEntry ã«åŸºã¥ã„ã¦ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
        :param entries: VectorStoreEntryã®ãƒªã‚¹ãƒˆï¼ˆfile_path ã¨ category ã‚’ä¿æŒï¼‰
        :param markdown_dir: Markdownå¤‰æ›å¾Œã®æ ¼ç´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        :param overwrite: æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¸Šæ›¸ãã™ã‚‹ã‹ã©ã†ã‹
        """
        for entry in entries:
            process_utils.process_and_vectorize_file(
                in_file=entry.file_path,
                markdown_dir=markdown_dir,
                category=entry.category,
                embedding_name=self.embedding_name,
                overwrite=overwrite
            )

    def prepare_chain(
        self,
        vectorstore_dir,
        category: retriever_utils.RetrieverCategory,
        k: int = 5
    ):
        """
        RetrieverCategory ã®ç¨®é¡ã«å¿œã˜ã¦ã€é©åˆ‡ãªãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
        """

        if isinstance(category, retriever_utils.HierarchicalRetrieverCategory):
            # éšå±¤å‹ã‚«ãƒ†ã‚´ãƒªã«å¯¾ã™ã‚‹å‡¦ç†
            self.qa_chain = chain_factory.prepare_chain_for_category(
                llm=self.llm,
                category=category,
                base_path=vectorstore_dir,
                chain_type="conversational",
                prompt_template=self.prompt_template,
                k=k,
            )

        elif isinstance(category, retriever_utils.FlatRetrieverCategory):
            # ãƒ•ãƒ©ãƒƒãƒˆå‹ã‚«ãƒ†ã‚´ãƒªã«å¯¾ã™ã‚‹å‡¦ç†ï¼ˆä¾‹ï¼‰
            self.qa_chain = chain_factory.prepare_flat_chain(
                llm=self.llm,
                category=category,
                base_path=vectorstore_dir,
                prompt_template=self.prompt_template,
                k=k,
            )

        else:
            raise TypeError(f"Unsupported category type: {type(category).__name__}")

    def run_interactive(self, mode: str = "rag"):
        """
        å¯¾è©±ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ

        Parameters:
        - mode: "rag"ï¼ˆRAGãƒã‚§ãƒ¼ãƒ³ä½¿ç”¨ï¼‰ã¾ãŸã¯ "llm"ï¼ˆLLMå˜ä½“ä½¿ç”¨ï¼‰

        Raises:
        - RuntimeError: å¯¾å¿œã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãŒæœªæ§‹ç¯‰ã®å ´åˆ
        """
        if mode == "rag":
            if not self.qa_chain:
                raise RuntimeError("RAGãƒã‚§ãƒ¼ãƒ³ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚prepare_chain() ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")
            
            print("ğŸ” RAG å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit'ï¼‰")
            while True:
                query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
                if query.strip().lower() == "exit":
                    break

                response = self.qa_chain.invoke({
                    "input": query,
                    "chat_history": []
                })

                print("\nğŸ“„ å‚ç…§å…ƒ:")
                for doc in response.get("source_documents", []):
                    print("  -", doc.metadata.get("source"))

                print("\nğŸ§  å›ç­”:")
                print(response.get("answer") or response.get("output"))

        elif mode == "llm":
            if not self.llm or not self.prompt_template:
                raise RuntimeError("LLMã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒæœªè¨­å®šã§ã™ã€‚")
            
            print("ğŸ’¬ LLM å˜ä½“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit'ï¼‰")
            while True:
                query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
                if query.strip().lower() == "exit":
                    break

                prompt = self.prompt_template.format_messages(
                    input=query,
                    context="ï¼ˆæ–‡è„ˆãªã—ï¼‰"
                )
                response = self.llm.invoke(prompt)

                print("\nğŸ§  å›ç­”:")
                print(response)

        else:
            raise ValueError(f"æœªçŸ¥ã®ãƒ¢ãƒ¼ãƒ‰ã§ã™: {mode}ï¼ˆ'rag' ã¾ãŸã¯ 'llm' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼‰")

