import os
import construct # type: ignore
import chain
import rag
from pathlib import Path
from langchain_ollama import OllamaLLM # type: ignore
from langchain.prompts import PromptTemplate # type: ignore
from langchain_core.prompts import ChatPromptTemplate # type: ignore


# --- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š ---
# ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰ã®ãƒ‘ã‚¹
current_path = Path(__file__).resolve()
 
# 'core' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å«ã‚€è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¦‹ã¤ã‘ã‚‹
core_root = next(p for p in current_path.parents if p.name == "core")

# ãã“ã‹ã‚‰ç›®çš„ã®ã‚µãƒ–ãƒ‘ã‚¹ã‚’å®šç¾©
base_dir = core_root / "sample"
markdown_dir = base_dir / "markdown"
vectorstore_dir = base_dir / "vectorstore"

# --- å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« ---
input_files = [
    markdown_dir / "cat_tools.md",
    markdown_dir / "japan_catapillar.md",
    markdown_dir / "Proffesional_College_of_arts_and_tourism.md",
]

# --- ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š ---
category = "test"

# --- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ ---
for in_file in input_files:
    try:
        md_path = rag.prepare_markdown(in_file, markdown_dir / in_file.name)  # å†åˆ©ç”¨ or å¤‰æ›å…ˆ
    except ValueError as e:
        print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {e}")
        continue
    basename = md_path.stem
    vect_path = vectorstore_dir / f"{basename}.faiss"
    construct.vectorization(md_path, vect_path, category=category)


# --- ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
llm = OllamaLLM(model="granite3.3:8b")

CUSTOM_PROMPT = ChatPromptTemplate.from_template("""
        ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€**æ—¥æœ¬èªã§**å°‚é–€çš„ã‹ã¤è«–ç†çš„ã«ã€ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚

        æ–‡è„ˆ:
        {context}

        è³ªå•:
        {input}
    """)

# --- ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰ï¼ˆã‚«ãƒ†ã‚´ãƒªæŒ‡å®šï¼‰ ---
qa_chain = chain.prepare_chain_for_category(
    llm=llm,
    category=category,
    base_path=vectorstore_dir,
    chain_type="conversational",
    prompt_template=CUSTOM_PROMPT,
    k=5,
)

# --- å¯¾è©±ãƒ«ãƒ¼ãƒ— ---
print("RAG å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ï¼‰")
while True:
    query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
    if query.strip().lower() == "exit":
        break

    response = qa_chain.invoke({
        "input": query,
        "chat_history": []
    })

    for doc in response.get("source_documents", []):
        print("ğŸ“„", doc.metadata.get("source"))

    print("\nğŸ§  å›ç­”:")
    print(response.get("answer") or response.get("output"))
