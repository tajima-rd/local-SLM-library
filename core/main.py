import os
import construct # type: ignore
import chain
import rag
from pathlib import Path
from langchain_ollama import OllamaLLM # type: ignore
from langchain.prompts import PromptTemplate # type: ignore
from langchain_core.prompts import ChatPromptTemplate # type: ignore


# --- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š ---
base_dir = Path("/home/yufujimoto/Documents/Projects/ç”Ÿæˆç³»AI/LocalLLM/webui/modules/core/sample")
markdown_dir = base_dir / "markdown"
vectorstore_dir = base_dir / "vectorstore"

# --- å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« ---
input_files = [
    base_dir / "cat_tools.md",
    base_dir / "japan_catapillar.md",
    base_dir / "Proffesional_College_of_arts_and_tourism.md",
]

# --- ã‚«ãƒ†ã‚´ãƒªæŒ‡å®š ---
category = "test"

# --- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ ---
for in_file in input_files:
    basename = in_file.stem
    md_path = markdown_dir / f"{basename}.md"
    vect_path = vectorstore_dir / f"{basename}.faiss"
    construct.vectorization(in_file, md_path, vect_path, category=category)

# --- ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
llm = OllamaLLM(model="granite3.2:8b")

CUSTOM_PROMPT = ChatPromptTemplate.from_template("""
        ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€**æ—¥æœ¬èªã§**å°‚é–€çš„ã‹ã¤è«–ç†çš„ã«ã€ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚

        æ–‡è„ˆ:
        {context}

        è³ªå•:
        {input}
    """)

# --- ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰ï¼ˆã‚«ãƒ†ã‚´ãƒªæŒ‡å®šï¼‰ ---
qa_chain = chain.prepare_chain_for_category(
    llm=llm,
    category=category,
    base_path=vectorstore_dir,
    chain_type="conversational",
    prompt_template=CUSTOM_PROMPT,
    k=5,
)

# --- å¯¾è©±ãƒ«ãƒ¼ãƒ— ---
print("RAG å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ï¼‰")
while True:
    query = input("\nğŸ—¨ è³ªå•ã—ã¦ãã ã•ã„: ")
    if query.strip().lower() == "exit":
        break

    response = qa_chain.invoke({
        "input": query,
        "chat_history": []
    })

    for doc in response.get("source_documents", []):
        print("ğŸ“„", doc.metadata.get("source"))

    print("\nğŸ§  å›ç­”:")
    print(response.get("answer") or response.get("output"))
